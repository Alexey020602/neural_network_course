{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Практическое занятие №2 - локализация объектов на изображениях\n",
        "\n",
        "1. Разметка данных с помощью виджета QSL для Jupyter (https://github.com/faustomorales/qsl)\n",
        "2. Расширение выборки за счет модификации размеченных данных (процедура аугоментация).\n",
        "3. Знакомство с архитектурой нейронных сетей для локализации объектов на изображении.\n",
        "4. Обучение нейронных сетей для детекции объектов.\n",
        "5. Проверка обученной модели для обработки видео потока.\n",
        "6. Трекинг объектов.\n",
        "7. Распознавание сцен с помощью простого классификатора.\n",
        "9. Разработка простой системы управления по данным с видео-камеры."
      ],
      "metadata": {
        "id": "nb3krk0emfCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![schema1.svg](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!-- Do not edit this file with editors other than diagrams.net -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="641px" height="61px" viewBox="-0.5 -0.5 641 61" content="&lt;mxfile host=&quot;Electron&quot; modified=&quot;2023-02-16T18:22:09.812Z&quot; agent=&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) draw.io/20.3.0 Chrome/104.0.5112.114 Electron/20.1.3 Safari/537.36&quot; etag=&quot;Aq33kz8ynVM5ex_2W5_t&quot; version=&quot;20.3.0&quot; type=&quot;device&quot;&gt;&lt;diagram id=&quot;NI0CinIh7Itd7kQ5NMls&quot; name=&quot;Страница 1&quot;&gt;7VfbjpswEP0aHitxCyGPC0t2VbVaVVupat+8wQG6hokcJyT79R2MDaYkaXrNKqoiTewzM7aZMwcnlheXuztOVvl7SCmzXDvdWd6t5brT0EXbAPsWmPizFsh4kbaQ0wOPxQtVoK3QTZHS9SBQADBRrIbgAqqKLsQAI5xDPQxbAhvuuiIZHQGPC8LG6KciFXmLhu60x+9pkeV6ZydQz1cSHayeZJ2TFGoD8hLLizmAaEflLqasqZ2uS5s3P+LtDsZpJc5J+Pzhzvv44ryNZuT+S/DgP6R19MZrV9kStlEPrA4r9roCNMWCqClwkUMGFWFJj0YcNlVKm21snPUx7wBWCDoIfqVC7BW7ZCMAoVyUTHnXz1Qscj0RHJ5pDAy4PICXBM0HPUuoxCG8PXJzzqOlUdAaNnxBT9RDtxjhGRUn4tyOQGx8CiUVfI95nDIiiu3wHES1YNbF9SzhQBH1E6Q5Y9Kw8jNpo1BaX9qJtImFdQ1bxJZ2Jm00onpIZJ0Xgj6uiCxYjeo+QdqyYExTU0FFf4PHLeWC7k4zOa68ft8osam3TSe+uteuo7Hc0G1g/yWuJn9eYIPC/4raLikw90yB+ZcUmHtUYCikSIrHMaSFvROQspFH9bRuvqTvxhCcr+JUDtpmfUxjWIPoieMoEzrRV+psUuZNWJ/b2qm0sZY4el1jx/AKZe36/ivT9fS/rsd6PUPXwSV17R/U9c352rKlHM8Tt9Lid/LFsSNtqG/rztve1rfGLT7R6xx/81y37n33td3nweGrIdHUICmetNMfcRobDWP+XIs0mwcvh9Boia4DI2N93+i0wOix+RW2x2T279oDp/1/Nekz/vB6yTc=&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><path d="M 120 30 L 167.63 30" fill="none" stroke="none" pointer-events="stroke"/><path d="M 172.88 30 L 165.88 33.5 L 167.63 30 L 165.88 26.5 Z" fill="none" stroke="none" pointer-events="all"/><path d="M 120 30 M 120 30 C 131.58 28.06 138.94 29.15 167.63 30 M 120 30 C 134.26 29.46 148.83 30.15 167.63 30" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 165.61 26.82 C 165.61 26.82 165.61 26.82 165.61 26.82 M 165.61 26.82 C 165.61 26.82 165.61 26.82 165.61 26.82 M 167.97 30.19 C 168.24 29.62 168.74 29.17 169.28 28.68 M 167.97 30.19 C 168.38 29.65 168.85 29.08 169.28 28.68" fill="none" stroke="#e6e6e6" stroke-width="0.5" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><path d="M 172.88 30 M 172.88 30 C 171.28 30.67 169.55 32.39 165.88 33.5 M 172.88 30 C 170.5 31.42 168.17 32.49 165.88 33.5 M 165.88 33.5 C 166.42 32.38 167.33 30.73 167.63 30 M 165.88 33.5 C 166.44 32.42 167.02 31.36 167.63 30 M 167.63 30 C 167.16 29.37 166.42 28.39 165.88 26.5 M 167.63 30 C 166.92 28.88 166.39 27.42 165.88 26.5 M 165.88 26.5 C 167.77 26.67 169.1 27.39 172.88 30 M 165.88 26.5 C 167.82 27.44 169.22 28.49 172.88 30" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><rect x="0" y="0" width="120" height="60" fill="none" stroke="none" pointer-events="all"/><path d="M 0 0 C 25.4 0.16 50.66 0.85 120 0 M 0 0 C 42.33 0.09 86.21 0.34 120 0 M 120 0 C 119.58 13.72 118.62 31.81 120 60 M 120 0 C 118.83 24.06 119.22 47.83 120 60 M 120 60 C 82.73 61.02 43.96 60.24 0 60 M 120 60 C 88.13 59.9 56.79 59.77 0 60 M 0 60 C -1.39 43.29 -1.16 24.17 0 0 M 0 60 C 0.94 47.42 0.71 33.05 0 0" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 1px;"><div data-drawio-colors="color: #E6E6E6; " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(230, 230, 230); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Видеофайл</div></div></div></foreignObject><text x="60" y="34" fill="#E6E6E6" font-family="Helvetica" font-size="12px" text-anchor="middle">Видеофайл</text></switch></g><path d="M 294 30 L 343.63 30" fill="none" stroke="none" pointer-events="stroke"/><path d="M 348.88 30 L 341.88 33.5 L 343.63 30 L 341.88 26.5 Z" fill="none" stroke="none" pointer-events="all"/><path d="M 294 30 M 294 30 C 303.72 30.77 317.16 32.26 343.63 30 M 294 30 C 306.48 30.27 318.02 29.39 343.63 30" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 341.96 26.41 C 341.96 26.41 341.96 26.41 341.96 26.41 M 341.96 26.41 C 341.96 26.41 341.96 26.41 341.96 26.41 M 343.66 30.55 C 344.69 29.56 345.23 28.93 345.63 28.28 M 343.66 30.55 C 344.24 29.79 344.82 29.28 345.63 28.28" fill="none" stroke="#e6e6e6" stroke-width="0.5" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><path d="M 348.88 30 M 348.88 30 C 345.93 32.01 343.32 32.21 341.88 33.5 M 348.88 30 C 347.53 31.14 346.05 31.71 341.88 33.5 M 341.88 33.5 C 342.46 32.78 342.59 31.47 343.63 30 M 341.88 33.5 C 342.14 32.76 342.61 31.98 343.63 30 M 343.63 30 C 343.12 29.47 342.78 28.56 341.88 26.5 M 343.63 30 C 342.87 28.78 342.37 27.68 341.88 26.5 M 341.88 26.5 C 344.65 27.52 348.01 29.32 348.88 30 M 341.88 26.5 C 344.08 27.84 347.15 28.82 348.88 30" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><rect x="174" y="0" width="120" height="60" fill="none" stroke="none" pointer-events="all"/><path d="M 174 0 C 198.26 0.08 226.34 0.97 294 0 M 174 0 C 202.17 -0.42 228.26 1.05 294 0 M 294 0 C 292.77 18 294.67 32.87 294 60 M 294 0 C 294.16 22.34 293.45 44.41 294 60 M 294 60 C 260.74 58.37 224.56 59.08 174 60 M 294 60 C 258.47 60.28 221.24 59.04 174 60 M 174 60 C 174.32 46.83 175.09 33.07 174 0 M 174 60 C 173.03 38.25 172.97 16.63 174 0" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 175px;"><div data-drawio-colors="color: #E6E6E6; " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(230, 230, 230); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Выбор кадров <br />для разметки</div></div></div></foreignObject><text x="234" y="34" fill="#E6E6E6" font-family="Helvetica" font-size="12px" text-anchor="middle">Выбор кадров...</text></switch></g><path d="M 470 30 L 513.63 30" fill="none" stroke="none" pointer-events="stroke"/><path d="M 518.88 30 L 511.88 33.5 L 513.63 30 L 511.88 26.5 Z" fill="none" stroke="none" pointer-events="all"/><path d="M 470 30 M 470 30 C 480.15 28.46 489.95 30.35 513.63 30 M 470 30 C 487.68 30.15 503.19 29.72 513.63 30" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 511.61 26.81 C 511.61 26.81 511.61 26.81 511.61 26.81 M 511.61 26.81 C 511.61 26.81 511.61 26.81 511.61 26.81 M 513.97 30.19 C 514.4 29.78 514.77 29.29 515.28 28.68 M 513.97 30.19 C 514.33 29.9 514.63 29.39 515.28 28.68" fill="none" stroke="#e6e6e6" stroke-width="0.5" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><path d="M 518.88 30 M 518.88 30 C 517.55 31.08 515.46 32.19 511.88 33.5 M 518.88 30 C 516.39 30.79 514.35 32.33 511.88 33.5 M 511.88 33.5 C 512.78 32.48 513.27 30.81 513.63 30 M 511.88 33.5 C 512.5 32.36 512.83 31.15 513.63 30 M 513.63 30 C 512.8 28.86 512.52 27.34 511.88 26.5 M 513.63 30 C 513.22 28.68 512.36 27.56 511.88 26.5 M 511.88 26.5 C 514.06 27.53 516.49 29.71 518.88 30 M 511.88 26.5 C 513.71 27.54 516.28 28.54 518.88 30" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><rect x="350" y="0" width="120" height="60" fill="none" stroke="none" pointer-events="all"/><path d="M 350 0 C 375.98 1.52 401.71 -1.19 470 0 M 350 0 C 393.84 0.96 440.36 0.87 470 0 M 470 0 C 468.34 22.56 471.96 42.98 470 60 M 470 0 C 470.83 18.91 469.91 39.58 470 60 M 470 60 C 444.76 61.06 421.77 60.77 350 60 M 470 60 C 423.13 61.04 378.13 59.58 350 60 M 350 60 C 352.15 41.9 349.97 22.88 350 0 M 350 60 C 350.41 43.91 350.68 27.78 350 0" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 351px;"><div data-drawio-colors="color: #E6E6E6; " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(230, 230, 230); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Разметка кадров и расширение выборки</div></div></div></foreignObject><text x="410" y="34" fill="#E6E6E6" font-family="Helvetica" font-size="12px" text-anchor="middle">Разметка кадров и расширение выборки</text></switch></g><rect x="520" y="0" width="120" height="60" fill="none" stroke="none" pointer-events="all"/><path d="M 520 0 C 547.69 -1.04 571.07 0.65 640 0 M 520 0 C 557.51 -0.06 596.46 0.28 640 0 M 640 0 C 640.71 15.12 638.06 29.09 640 60 M 640 0 C 639.49 15.48 640.38 32.75 640 60 M 640 60 C 598.78 59.75 560.98 62.46 520 60 M 640 60 C 607.8 61.8 575.02 60.11 520 60 M 520 60 C 518.77 48.98 521.65 36.69 520 0 M 520 60 C 519.78 37.56 520.4 14.93 520 0" fill="none" stroke="#e6e6e6" stroke-linejoin="round" stroke-linecap="round" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 521px;"><div data-drawio-colors="color: #E6E6E6; " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(230, 230, 230); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Обучение модели<br />и валидация</div></div></div></foreignObject><text x="580" y="34" fill="#E6E6E6" font-family="Helvetica" font-size="12px" text-anchor="middle">Обучение модели...</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Text is not SVG - cannot display</text></a></switch></svg>)"
      ],
      "metadata": {
        "id": "cX355oU6dsDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "%%html\n",
        "<video controls width=\"250\">\n",
        "    <source src=\"https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_1/video_1.mp4\" type=\"video/mp4\"/>\n",
        "</video>\n",
        "<video controls width=\"250\">\n",
        "    <source src=\"https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/video_3.mp4\" type=\"video/mp4\"/>\n",
        "</video>\n",
        "<video controls width=\"250\">\n",
        "    <source src=\"https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_3/video_4.mp4\" type=\"video/mp4\"/>\n",
        "</video>\n",
        "<video controls width=\"250\">\n",
        "    <source src=\"https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/video_4.mp4\" type=\"video/mp4\"/>\n",
        "</video>"
      ],
      "metadata": {
        "id": "TAoDS5yTzL4h",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка дополнительных пакетов для работы с изображениями"
      ],
      "metadata": {
        "id": "1PgHXo7qBmiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-image==0.19.3"
      ],
      "metadata": {
        "id": "A66ogp06LgY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations==1.3.0"
      ],
      "metadata": {
        "id": "Isp1jEaFwF18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q qsl"
      ],
      "metadata": {
        "id": "_E0KvHivgWLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Разметка кадров для обучающей выборки"
      ],
      "metadata": {
        "id": "m6q5-uEtAaME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Активация расширенных виджетов в Colab (можно закомментировать при запуске в Jupyter)\n",
        "import google.colab\n",
        "google.colab.output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "fC89lkkqhY_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.arrays import boolean\n",
        "from dataclasses import dataclass\n",
        "import requests\n",
        "import numpy\n",
        "import cv2\n",
        "from collections import namedtuple\n",
        "import typing\n",
        "from skimage.filters import butterworth\n",
        "import itertools\n",
        "import albumentations as aug\n",
        "import json\n",
        "import pandas\n",
        "\n",
        "import plotly.express as plte\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import qsl"
      ],
      "metadata": {
        "id": "XonId0u4hWC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile images.csv\n",
        "target\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_1/image_001.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_1/image_002.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_1/image_003.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/image_005.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/image_006.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/image_007.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/image_008.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/image_009.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_2/image_010.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_3/image_011.jpg"
      ],
      "metadata": {
        "id": "PF5QcZa25CqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file = pandas.read_csv('images.csv')\n",
        "image_file.to_dict(orient='records')"
      ],
      "metadata": {
        "id": "oa6Stj2FAhcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeler = qsl.MediaLabeler(\n",
        "    items=image_file.to_dict(orient='records'),\n",
        "    config={\n",
        "        \"regions\": [\n",
        "            {\"name\": \"Object\", \"multiple\": True, \"options\": [{\"name\": \"bottle\"}, {\"name\": \"bag\"}]}\n",
        "        ]\n",
        "    })\n",
        "labeler.mode = \"dark\"\n",
        "labeler"
      ],
      "metadata": {
        "id": "GBJDJcy6hN4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('result.json', 'w') as file:\n",
        "    file.write(json.dumps(labeler.items, sort_keys=True, indent=4))"
      ],
      "metadata": {
        "id": "pCInWsS8kEUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = None\n",
        "with open('result.json', 'r') as file:\n",
        "    labels = file.read()\n",
        "    print(labels)"
      ],
      "metadata": {
        "id": "I-1SxmA2DyA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O object_masks.json 'https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/object_masks.json'"
      ],
      "metadata": {
        "id": "DPDvrPq-iRyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = None\n",
        "with open('object_masks.json', 'r') as file:\n",
        "    labels = file.read()\n",
        "label_data = json.loads(labels)"
      ],
      "metadata": {
        "id": "BTShllOuEFto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeler = qsl.MediaLabeler(\n",
        "    items=label_data,\n",
        "    config={\n",
        "        \"regions\": [\n",
        "            {\"name\": \"Type\", \"multiple\": True, \"options\": [{\"name\": \"bottle\"},\n",
        "                                                           {\"name\": \"bag\"}]}\n",
        "        ]\n",
        "    })\n",
        "labeler.labels\n",
        "labeler.mode = \"dark\"\n",
        "labeler"
      ],
      "metadata": {
        "id": "vK_6h2AX2ySU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Нарезка изображений по объектам"
      ],
      "metadata": {
        "id": "v85Ib-sB0RZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Struct(object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        for name, value in data.items():\n",
        "            setattr(self, name, self._wrap(value))\n",
        "\n",
        "    def _wrap(self, value):\n",
        "        if isinstance(value, (tuple, list, set, frozenset)): \n",
        "            return type(value)([self._wrap(v) for v in value])\n",
        "        else:\n",
        "            return Struct(value) if isinstance(value, dict) else value\n",
        "\n",
        "    def __str__(self):\n",
        "        result = '{'\n",
        "        first = True\n",
        "        for field in self.__dict__.keys():\n",
        "            if not first:\n",
        "                result += ', '\n",
        "            else:\n",
        "                first = False\n",
        "            result += f'{field}: {str(self.__dict__[field])}'\n",
        "        return result + '}'\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)"
      ],
      "metadata": {
        "id": "_Wv-oeUGdVio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [Struct(label) for label in label_data]"
      ],
      "metadata": {
        "id": "KA4YgYr4ivpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RawObject = namedtuple('RawObject', ['image_url', 'mask_polygon', 'label'])\n",
        "Point = namedtuple('Point', ['x', 'y'])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Box:\n",
        "    cx: float\n",
        "    cy: float\n",
        "    w: float\n",
        "    h: float\n",
        "\n",
        "    def __add__(self, p: Point):\n",
        "        return Box(cx=self.cx + p.x, cy=self.cy + p.y, w=self.w, h=self.h)\n",
        "\n",
        "    @property\n",
        "    def x1(self):\n",
        "        return self.cx - self.w/2\n",
        "\n",
        "    @property\n",
        "    def x2(self):\n",
        "        return self.cx + self.w/2\n",
        "\n",
        "    @property\n",
        "    def y1(self):\n",
        "        return self.cy - self.h/2\n",
        "\n",
        "    @property\n",
        "    def y2(self):\n",
        "        return self.cy + self.h/2\n",
        "\n",
        "\n",
        "def make_default_background(shape):\n",
        "    back = numpy.random.normal(loc=128, scale=100, size=shape).astype(numpy.uint8)\n",
        "    back = butterworth(back, 0.04, False, 4, channel_axis=-1)\n",
        "    return back\n",
        "\n",
        "\n",
        "class MaskedObjectImage:\n",
        "\n",
        "    def __init__(self, image, mask, label):\n",
        "        self.orig_image = image\n",
        "        self.mask = mask\n",
        "        self.label = label\n",
        "        self.__masked = None\n",
        "\n",
        "    @property\n",
        "    def masked(self):\n",
        "        background = make_default_background(self.orig_image.shape)\n",
        "        masked = self.orig_image.copy()\n",
        "        index = self.mask == 0\n",
        "        masked[index] = background[index]\n",
        "        return masked\n",
        "\n",
        "    @property\n",
        "    def image(self):\n",
        "        return self.orig_image\n",
        "\n",
        "    def place(self, background: numpy.ndarray,\n",
        "              pos: typing.Tuple = (0, 0), copy: boolean = True):\n",
        "        result = None\n",
        "        if copy:\n",
        "            result = background.copy()\n",
        "        else:\n",
        "            result = background\n",
        "\n",
        "        if pos[0] > background.shape[1] or pos[1] > background.shape[0]:\n",
        "            raise Exception(\n",
        "                'Invalid position={} for background.shape={} (out of range)'\n",
        "                .format(pos, background.shape))\n",
        "\n",
        "        index = self.mask == 0\n",
        "        start_x = pos[0]\n",
        "        start_y = pos[1]\n",
        "        result_end_x = None\n",
        "        result_end_y = None\n",
        "        image_end_x = None\n",
        "        image_end_y = None\n",
        "\n",
        "        if start_x + self.mask.shape[1] <= result.shape[1]:\n",
        "            result_end_x = start_x + self.mask.shape[1]\n",
        "            image_end_x = self.mask.shape[1]\n",
        "        else:\n",
        "            result_end_x = result.shape[1]\n",
        "            image_end_x = result.shape[1] - start_x\n",
        "\n",
        "        if start_y + self.mask.shape[0] <= result.shape[0]:\n",
        "            result_end_y = start_y + self.mask.shape[0]\n",
        "            image_end_y = self.mask.shape[0]\n",
        "        else:\n",
        "            result_end_y = result.shape[0]\n",
        "            image_end_y = result.shape[0] - start_y\n",
        "\n",
        "        copy = result[start_y:result_end_y, start_x:result_end_x, :].copy()\n",
        "        index = self.mask > 0\n",
        "        index = index[:image_end_y, :image_end_x]\n",
        "        copy[index] = self.orig_image[:image_end_y, :image_end_x, :][index]\n",
        "        result[start_y:result_end_y, start_x:result_end_x, :] = copy\n",
        "\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def box(self):\n",
        "        index = numpy.where(\n",
        "            self.mask > 0\n",
        "        )\n",
        "        min_x = numpy.min(index[1])\n",
        "        max_x = numpy.max(index[1])\n",
        "        min_y = numpy.min(index[0])\n",
        "        max_y = numpy.max(index[0])\n",
        "        w = max_x - min_x\n",
        "        h = max_y - min_y\n",
        "        cx = min_x + w/2\n",
        "        cy = min_y + h/2\n",
        "\n",
        "        return Box(\n",
        "            cx=cx,\n",
        "            cy=cy,\n",
        "            w=w,\n",
        "            h=h)\n",
        "\n",
        "\n",
        "class MaskedObjectsDataset:\n",
        "\n",
        "    def __init__(self, metadata: typing.List[Struct]):\n",
        "        self.metadata = metadata\n",
        "        self.objects_meta = [\n",
        "            RawObject(image_url=image.target,\n",
        "                      label=obj.labels.Type[0],\n",
        "                      mask_polygon=pandas.DataFrame.from_records(\n",
        "                          [vars(point) for point in obj.points]))\n",
        "            for image in self.metadata\n",
        "            for obj in image.labels.polygons\n",
        "            ]\n",
        "        self.basic_images = {}\n",
        "        self.objects = {}\n",
        "        s = sorted(set(itertools.chain.from_iterable(\n",
        "            [item.labels.Type for meta in metadata \n",
        "             for item in meta.labels.polygons])))\n",
        "        self.label2class = {s[i]: i + 1 for i in range(0, len(s))}\n",
        "        self.class2label = {i + 1: s[i] for i in range(0, len(s))}\n",
        "        self.label2class['background'] = 0\n",
        "        self.class2label[0] = 'background'\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.objects_meta)\n",
        "\n",
        "    def __getitem__(self, value) -> MaskedObjectImage:\n",
        "        if not isinstance(value, slice):\n",
        "            value = slice(value, value + 1)\n",
        "\n",
        "        start = value.start\n",
        "        if start is None:\n",
        "            start = 0\n",
        "        stop = value.stop\n",
        "        if stop is None:\n",
        "            stop = len(self)\n",
        "        step = value.step\n",
        "        if step is None:\n",
        "            step = 1\n",
        "\n",
        "        result = []\n",
        "        for i in range(start, stop, step):\n",
        "            result.append(self.__get_single_item(i))\n",
        "\n",
        "        if len(result) == 1:\n",
        "            result = result[0]\n",
        "\n",
        "        return result\n",
        "        \n",
        "    def __get_single_item(self, i):\n",
        "        if i not in self.objects:\n",
        "            self.__create_object(i)\n",
        "\n",
        "        return self.objects[i]\n",
        "\n",
        "    def __create_object(self, i):\n",
        "        object_meta = self.objects_meta[i]\n",
        "        image = None\n",
        "        if not object_meta.image_url in self.basic_images:\n",
        "            responce = requests.get(object_meta.image_url)\n",
        "            if not responce.status_code == 200:\n",
        "                raise Exception(\"Error, cant get image {}: {}\".format(\n",
        "                    object_meta.image_url, responce.status_code))\n",
        "            raw_bytes = numpy.frombuffer(responce.content, dtype=numpy.uint8)\n",
        "            self.basic_images[object_meta.image_url] = cv2.cvtColor(\n",
        "                cv2.imdecode(raw_bytes, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        basic_image = self.basic_images[object_meta.image_url]\n",
        "        min_x = int(object_meta.mask_polygon['x'].min()*basic_image.shape[1])\n",
        "        min_y = int(object_meta.mask_polygon['y'].min()*basic_image.shape[0])\n",
        "        max_x = int(object_meta.mask_polygon['x'].max()*basic_image.shape[1])\n",
        "        max_y = int(object_meta.mask_polygon['y'].max()*basic_image.shape[0])\n",
        "        image = basic_image[min_y:max_y, min_x:max_x, :]\n",
        "        mask = numpy.zeros(image.shape, dtype=numpy.uint8)\n",
        "        object_meta.mask_polygon['nx'] = (\n",
        "            object_meta.mask_polygon['x']*basic_image.shape[1]).map(int) - min_x\n",
        "        object_meta.mask_polygon['ny'] = (\n",
        "            object_meta.mask_polygon['y']*basic_image.shape[0]).map(int) - min_y\n",
        "        cv2.fillPoly(mask, numpy.expand_dims(\n",
        "            object_meta.mask_polygon[['nx', 'ny']].to_numpy(), axis=0),\n",
        "            (255, 255, 255))\n",
        "        self.objects[i] = MaskedObjectImage(\n",
        "            image=image,\n",
        "            mask=mask,\n",
        "            label=self.label2class[object_meta.label])\n"
      ],
      "metadata": {
        "id": "mceSP2_z6FbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %debug\n",
        "dataset = MaskedObjectsDataset(metadata=labels)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "S2uFqDU0eehM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "\n",
        "def plot_masked_objects(\n",
        "    images: typing.Union[MaskedObjectImage, typing.List[MaskedObjectImage]],\n",
        "    labels: typing.Dict[int, str]) -> None:\n",
        "    if isinstance(images, MaskedObjectImage):\n",
        "        images = [images]\n",
        "    for img in images:\n",
        "        fig = make_subplots(\n",
        "            horizontal_spacing=0.0, vertical_spacing=0.0,\n",
        "            rows=1, cols=3, shared_yaxes=True, shared_xaxes=True,\n",
        "            subplot_titles=[f\"{labels[img.label]}[{str(img.label)}]\"])\n",
        "        fig.add_trace(go.Image(z=img.orig_image), row=1, col=1)\n",
        "        fig.add_trace(go.Image(z=img.mask), row=1, col=2)\n",
        "        fig.add_trace(go.Image(z=img.masked), row=1, col=3)\n",
        "        fig.show()"
      ],
      "metadata": {
        "id": "tQtcyI5s6lo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_masked_objects(dataset[0], dataset.class2label)"
      ],
      "metadata": {
        "id": "m2zisjQOKh76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Расширение обучающей выборки \n",
        "\n",
        "На практике не всегда удается собрать достаточно размеченного материала для качественного обучения нейронной сети. Кроме того, использование большого массива изображений без должного анализа состава приводит к разбалансировке процесса обучения. Например, в общем потоке изображений какие-либо классы объектов могут быть представлены незначительным числом примеров и по этой причине их распознавание во время обучения будет иметь малую значимость на фоне остальных классов. В этой связи все современные модели при обучении предполагают в определенной степени крапотливое конструирование синтетического набора данных на основе исходных изображений. Подробный обзор стратегий расширения выборки за счет изменения исходных изображений представлен в статье \"A Comprehensive Survey of Image Augmentation Techniques for Deep Learning\" (https://arxiv.org/abs/2205.01491).\n",
        "\n",
        "Далее будем использовать следующий способ расширения выборки изображений.\n",
        "1. Извлечение изображений интересующих нас объектов из исходных данных.\n",
        "2. Создание набора фоновых изображений для размещения объектов.\n",
        "3. Определение набора трасформаций изображений объектов и фона, включая масштабирование, отражения, повороты, обрезку, изменение цвета/насыщенности, наложение шума.\n",
        "4. Размещение объектов на фоне в случайных местах.\n",
        "\n",
        "В настоящее время нет необходимости реализовывать с нуля все шаги синтеза изображений, т.к. существуюет большой набор готовых решений. Наиболее популярными решениями служат следующие пакеты Python.\n",
        "\n",
        "1. https://pytorch.org/vision/stable/transforms.html\n",
        "2. https://www.tensorflow.org/tutorials/images/data_augmentation?hl=en\n",
        "3. https://augmentor.readthedocs.io/en/master/\n",
        "4. https://github.com/albumentations-team/albumentations\n",
        "\n",
        "Важным вопросом при расширении выборки является выбор параметров. Он может быть выполнен как эмпирически, так и путем оптимизации. В последнем случае используют либо целевую модель, либо её малый эквивалент с целью вариации параметров и оценки качества обучения. Логичным итогом такого рода работы становятся так называемые \"системы автоаугоментации\", в которые встраивается универсальная внутренняя модель и алгоритм оптимизации для подбора параметров трансформаций. Они могут использоваться для расширения новых наборов данных без дополнительных усилий со стороны разработчика (но ценой значительного расхода вычислений, т.к. необходимо многократно проводить процесс обучения моделей). Более подробно об основной идее такого автоматического подхода можно почитать в статье сотрудников компании Google (https://arxiv.org/abs/1805.09501), а также изучить похожую реализацию в пакете deepaugment - https://github.com/barisozmen/deepaugment (к сожалению, пакет больше не поддерживается, последнее обновление исходного кода было в 2019 году).\n",
        "Далее будем использовать более простой \"эмпирический способ\".\n",
        "\n"
      ],
      "metadata": {
        "id": "h2oYEOOTtSGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = aug.Compose([\n",
        "    aug.CropAndPad(percent=(0.2, 0.2), keep_size=False,\n",
        "                   pad_mode=cv2.BORDER_CONSTANT),\n",
        "    aug.LongestMaxSize(max_size=300),\n",
        "    aug.Rotate(p=0.9, border_mode=cv2.BORDER_CONSTANT,\n",
        "               value=(0, 0, 0), mask_value=(0, 0, 0)),\n",
        "    # aug.RandomCrop(width=50, height=50, p=0.25),\n",
        "    aug.HorizontalFlip(p=0.5),\n",
        "    aug.VerticalFlip(p=0.5),\n",
        "    aug.RandomScale(scale_limit=0.5, always_apply=True),\n",
        "    aug.MotionBlur(),\n",
        "    aug.RGBShift(always_apply=True,\n",
        "                 r_shift_limit=32, g_shift_limit=32, b_shift_limit=32, p=0.9),\n",
        "    aug.ChannelShuffle(p=0.3), # new\n",
        "    aug.HueSaturationValue(always_apply=True, p=1.0),\n",
        "    aug.RandomBrightnessContrast(p=0.2),\n",
        "    aug.GaussNoise(always_apply=True)\n",
        "])"
      ],
      "metadata": {
        "id": "TgV2G6NZ5sBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def sample_image(\n",
        "    images: typing.List[MaskedObjectImage],\n",
        "    transf: aug.Compose) -> MaskedObjectImage:\n",
        "    choice = int(random.random()*len(images))\n",
        "    sample = transf(image=images[choice].image, mask=images[choice].mask)\n",
        "    return MaskedObjectImage(image=sample['image'],\n",
        "                             mask=sample['mask'],\n",
        "                             label=images[choice].label)"
      ],
      "metadata": {
        "id": "7GQM-mEu0qLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_samples = 1\n",
        "plot_masked_objects([\n",
        "    sample_image(images=[dataset[1:2]], transf=transform)\n",
        "    for i in range(0, num_of_samples)], dataset.class2label)"
      ],
      "metadata": {
        "id": "mmdDqWmc2GYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_objects_image(\n",
        "    objects: MaskedObjectsDataset,\n",
        "    size: typing.Tuple[int, int, int],\n",
        "    N: int,\n",
        "    transformation: aug.Compose,\n",
        "    background: numpy.ndarray = None\n",
        "    ) -> typing.Tuple[numpy.ndarray, typing.List[Box], typing.List[int]]:\n",
        "\n",
        "    if background is None:\n",
        "        background = butterworth(make_default_background(size), 0.1, False, 4,\n",
        "                             channel_axis=-1)\n",
        "    result = background\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for i in range(0, N):\n",
        "        image = sample_image(images=objects, transf=transformation)\n",
        "        pos = Point(\n",
        "            x=int(random.random()*(result.shape[1] - image.mask.shape[1])),\n",
        "            y=int(random.random()*(result.shape[0] - image.mask.shape[0])))\n",
        "        result = image.place(\n",
        "            background=result,\n",
        "            pos=pos\n",
        "        )\n",
        "        boxes.append(image.box + pos)\n",
        "        labels.append(image.label)\n",
        "                \n",
        "    return result, boxes, labels\n",
        "\n",
        "\n",
        "def plot_image_with_objects(\n",
        "    image: MaskedObjectImage,\n",
        "    boxes: Box,\n",
        "    labels: typing.List[int],\n",
        "    labels_map: typing.Dict[int, str]) -> None:\n",
        "    f = plte.imshow(image)\n",
        "    for i in range(0, len(boxes)):\n",
        "        b = boxes[i]\n",
        "        label = labels_map[labels[i]]\n",
        "        f.add_trace(go.Scatter(\n",
        "            x=[b.x1, b.x1, b.x2, b.x2, b.x1],\n",
        "            y=[b.y1, b.y2, b.y2, b.y1, b.y1],\n",
        "            name=label))\n",
        "        f.add_annotation(x=b.x1, y=b.y1,\n",
        "                text=label,\n",
        "                showarrow=False,\n",
        "                yshift=10)\n",
        "    f.show()"
      ],
      "metadata": {
        "id": "JtOGI2-kL19s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, image_boxes, image_labels = generate_objects_image(\n",
        "    objects=dataset,\n",
        "    size=(768, 1280, 3),\n",
        "    N=10,\n",
        "    transformation=transform\n",
        ")\n",
        "plot_image_with_objects(\n",
        "    image=image,\n",
        "    boxes=image_boxes,\n",
        "    labels=image_labels,\n",
        "    labels_map=dataset.class2label\n",
        ")"
      ],
      "metadata": {
        "id": "7n8xD8q2001q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка выборки для валидации моделей\n",
        "Как и в первом практическом занятии необходим набор примеров для проверки качества обучаемой модели, а также определения оптимальных параметров процесса настройки весов для исключения переобучения."
      ],
      "metadata": {
        "id": "Z0o89DaMU_AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile images-validation.csv\n",
        "target\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image012.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image013.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image014.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image015.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image016.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image017.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image018.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image019.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image020.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/image021.jpg"
      ],
      "metadata": {
        "id": "JwCPixfGU-fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "image_validation_file = pandas.read_csv('images-validation.csv')\n",
        "image_validation_file"
      ],
      "metadata": {
        "id": "yzugKEFaWhD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import qsl\n",
        "\n",
        "labeler = qsl.MediaLabeler(\n",
        "    items=image_validation_file.to_dict(orient='records'),\n",
        "    config={\n",
        "        \"regions\": [\n",
        "            {\"name\": \"Object\", \"multiple\": True, \"options\": [{\"name\": \"bottle\"},\n",
        "                                                             {\"name\": \"bag\"}]}\n",
        "        ]\n",
        "    })\n",
        "labeler.mode = \"dark\"\n",
        "labeler"
      ],
      "metadata": {
        "id": "xFjtc9v0Wq4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('result-validation.json', 'w') as file:\n",
        "    file.write(json.dumps(labeler.items, sort_keys=True, indent=4))"
      ],
      "metadata": {
        "id": "N1BE4nrSjP6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O object-validation.json 'https://raw.githubusercontent.com/ant-nik/neural_network_course/main/practice_2_data/validation.json'"
      ],
      "metadata": {
        "id": "uVwBXD-mmEQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = None\n",
        "with open('object-validation.json', 'r') as file:\n",
        "    labels = file.read()\n",
        "validation_labels = json.loads(labels)"
      ],
      "metadata": {
        "id": "Hz4mDRFOmx8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeler = qsl.MediaLabeler(\n",
        "    items=validation_labels,\n",
        "    config={\n",
        "        \"regions\": [\n",
        "            {\"name\": \"Object\", \"multiple\": True, \"options\": [{\"name\": \"bottle\"},\n",
        "                                                             {\"name\": \"bag\"}]}\n",
        "        ]\n",
        "    })\n",
        "labeler.mode = \"dark\"\n",
        "labeler"
      ],
      "metadata": {
        "id": "uJEGPUpFnD6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Использование предобученной модели Fastern R-CNN для детекции\n",
        "\n",
        "Классическая задача детекции состоит в поиске объектов на изображении. Основные идеи детекции с помощью нейронных сетей группируются в семейства моделей. Одно из таких семейств использует идею генерации кандидатов детектируемых объектов по карте признаков, расчитываемой с помощью сети для задач классификации. Результатирующее положение и принадлежность объекта классу вычисляется с помощью полносвязной сети на основе списка кандидатов и карты признаков.\n",
        "\n",
        "Ключевой идеей является генерация подходящего набора канидатов по исходному изображению (RoI-region of interest). Задача осложняется многообразием объектов, которые необходимо находить на изображении, различных положений и соотношения сторон, а также переменным числом сущностей на изображении. \n",
        "\n",
        "Первой идеей таких моделей является использование фиксированного набора кандидатов с такими признаком как \"отсутствие объекта\". На выход всегда выдается одинаковое число объектов, но за счет фильтрации по признаку \"отсутствие объекта\" получается переменное число, которое не может превышать  максимально заложенное в модели.\n",
        "\n",
        "Второй идеей является использование фиксированного набора \"опорных областей интереса\" на изображении для детектируемых объектов с различным масштабированием и отношением сторон (Region Proposal - RP). Они задают дополнительную размерность в карте признаков и позволяют эффективнее вычислять итоговое положение и размер объектов по сравнению с прямым расчетом. Модель всего лишь корректирует параметры опорных областей, что гораздо эффективнее расчета абсолютных координат и размеров объектов на изображении. Таким образом, в карте признаков на каждый \"пиксель\" приходится $k$ кандидатов областей, для каждой из которых задается вероятность нахождения объекта и его класс (в том чсиле отсутствие какого-либо объекта). На практике использование опорных областей в изображении оказывается универсальнее и эффективнее с точки зрения числа параметров и вычислительной сложности по сравнению с предварительной генерацией достаточного числа произвольных прямоугольников и их классификации на предмет наличия объектов внутри.\n",
        "\n",
        "Третья идея состоит в переиспользовании результатов наиболее трудоемких вычислений различными подсистемами модели. Речь идёт о вычислении карты признаков исходного изображения на основе классических архитектур нейросетей для классификации изображений с отключенным финальным полносвязным слоем.\n",
        "\n",
        "https://arxiv.org/pdf/1908.03673.pdf\n",
        "https://paperswithcode.com/sota/object-detection-on-coco-2017-val\n",
        "\n",
        "| Модель | Год | Тип | Ключевые идеи | Особенности |\n",
        "| :- | :--: | :--: | :- | :- |\n",
        "| R-CNN | | двухшаговая на основе <br> областей интереса |  Selective Search для RoI + CNN для создания <br> feature map в каждом RoI + SVM для классификации | Медленная работа, фиксированный алгоритм генерации RoI|\n",
        "| Fast R-CNN |  | двухшаговая на основе <br> областей интереса | CNN (feature map) расчитывается однократно, <br> далее нарезается по итогам работы Selective Search на <br> Feature Map для каждого RoI, затем слой RoI Pooling с окном <br> фиксированного размера и заключительные две FC-сети для <br> классификации и коррекции RoI в ограничивающий <br> прямоугольник объекта| ~ в 25 раз быстрее R-CNN, но проблема с Selective Search осталась|\n",
        "| Fastern R-CNN | [2015](https://arxiv.org/abs/1506.01497) | двухшаговая на основе <br> областей интереса |Замена Selective Search на Region Proposal Network: <br> вместо эвристического алгоритма используется вложенная <br> нейронная сеть (RPN), которая вычисляет коррекцию <br> фиксированного набора опорных прямоугольников| ~ в 10 раз быстрее Fast R-CNN (~5-10 FPS)|\n",
        "|SSD|[2015](https://arxiv.org/abs/1512.02325)|одношаговая (по сетке?)||\n",
        "|RetinaNet | [2017](https://arxiv.org/abs/1708.02002) | одношаговая (по сетке?) | Точнее Yolo, быстрее Faster R-CNN, но проигрывает ей в точности|\n",
        "| Yolo | [2015-2023](https://arxiv.org/pdf/2301.05586v1.pdf) | одношаговая по сетке | You Only Look Once - разбивка изображения на сетку, <br> использвоание комбинации различных техник для создания | Наиболее быстрая модель ~ в 10 раз быстрее Faster-R-CNN <br> (~50-150 FPS на GPU), однако могут быть проблемы в детекции объектов <br> разных масштабов |\n",
        "| FCos | [2019](https://arxiv.org/abs/1904.01355) | одношаговая (?) без сетки||\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qAlslwHXKC-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from torchvision.io.image import read_image\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "!lscpu\n",
        "print(f'GPU count = {torch.cuda.device_count() if torch.cuda.is_available() else \"0\"}')"
      ],
      "metadata": {
        "id": "S7AfQ2v_UgXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "kHtrUOKLux6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "HDVo2orZ2so1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = torch.from_numpy(\n",
        "    numpy.moveaxis(image.astype(numpy.uint8), 2, 0))\n",
        "\n",
        "# Step 1: Initialize model with the best available weights\n",
        "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Step 2: Initialize the inference transforms\n",
        "preprocess = weights.transforms().to(device)"
      ],
      "metadata": {
        "id": "n0O8GhK9HVPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Apply inference preprocessing transforms\n",
        "batch = [preprocess(img).to(device)]"
      ],
      "metadata": {
        "id": "KieJSUEQiCN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Use the model and visualize the prediction\n",
        "result = model(batch)"
      ],
      "metadata": {
        "id": "Bxh1eEwTLYXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %debug\n",
        "box = img\n",
        "for res in result:\n",
        "    prediction = res\n",
        "    pred_labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"].tolist()]\n",
        "    box = draw_bounding_boxes(box, boxes=prediction[\"boxes\"],\n",
        "                              labels=pred_labels,\n",
        "                              colors=\"red\",\n",
        "                              width=4, font_size=30)\n",
        "\n",
        "fig = plte.imshow(to_pil_image(box.detach()))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "UC8kyiu_NGHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "PV1IOUN324uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "vN44CUlMIeW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model, input_data=[batch])"
      ],
      "metadata": {
        "id": "krSx8uCeiJlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InputOutput = namedtuple('InputOutput', ['input', 'output'])\n",
        "\n",
        "def get_activation(name, trace):\n",
        "    def hook(model, input, output):\n",
        "        trace[name] = InputOutput(input[0].detach(), output.detach())\n",
        "\n",
        "    return hook"
      ],
      "metadata": {
        "id": "2IRAYl_Trks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trace = {}\n",
        "model.roi_heads.box_head[5].register_forward_hook(get_activation('head-5', trace))\n",
        "model.roi_heads.box_head[6].register_forward_hook(get_activation('head-6', trace))\n",
        "model.roi_heads.box_predictor.cls_score.register_forward_hook(get_activation('cls', trace))\n",
        "model.roi_heads.box_predictor.bbox_pred.register_forward_hook(get_activation('bbox', trace))\n",
        "model.rpn.head.cls_logits.register_forward_hook(get_activation('cls_log_prop', trace))\n",
        "model.rpn.head.bbox_pred.register_forward_hook(get_activation('bbox_prop', trace))\n",
        "output = model(batch)\n",
        "output"
      ],
      "metadata": {
        "id": "xr5ZbMlO7X6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in trace.items():\n",
        "    print(f\"{key}: {value.input.shape} => {value.output.shape}\")"
      ],
      "metadata": {
        "id": "wJ3LGtFTY-22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "threshold = 0.75\n",
        "cls = trace['cls'].output[:, 1:]\n",
        "cls_prob_all = F.softmax(cls, -1)\n",
        "cls_prob_thr = (cls_prob_all > threshold).float()\n",
        "cls_prob_x = cls_prob_all * cls_prob_thr\n",
        "\n",
        "obji = torch.argmax(cls_prob_x, 1)\n",
        "bboxi = cls_prob_all[range(cls_prob_all.shape[0]), obji]\n",
        "bboxes = trace['bbox'].output[bboxi > threshold]\n",
        "clses = obji[bboxi > threshold]\n",
        "conf = cls_prob_all[range(cls_prob_all.shape[0]), obji][bboxi > threshold]"
      ],
      "metadata": {
        "id": "4PA4YjtGdil6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[{\n",
        "    'class': i + 1, 'label': weights.meta[\"categories\"][i + 1],\n",
        "    'count': len(conf[clses == i]),\n",
        "    'prob': f'{int(min(conf[clses == i]).item()*100)}-{int(max(conf[clses == i]).item()*100)}%'\n",
        " } for i in clses.unique().tolist()]"
      ],
      "metadata": {
        "id": "GGV1LVtcF2vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: \n",
        "1. Понять почему так много объектов, возможно кого-то нужно отсекать по размеру bbox? (разобраться по исходникам класса).\n",
        "2. Разобраться как вычисляются координаты bbox из bbox-регресии для того чтобы реконструировать вычисление выхода сетки из промежуточных результатов trace['bbox'] и обновить таблицу выше."
      ],
      "metadata": {
        "id": "Q3EEr3YCF96I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Изменение числа классов в Faster R-CNN"
      ],
      "metadata": {
        "id": "8W-98zBbUg6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "tunned_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "DKyKGUrmWLW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in tunned_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "ANxhjXrzXkVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.roi_heads.box_predictor.cls_score)\n",
        "print(model.roi_heads.box_predictor.bbox_pred)"
      ],
      "metadata": {
        "id": "P55d5bKzXzfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "\n",
        "num_of_classes = 2\n",
        "tunned_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "    in_channels=1024, num_classes=num_of_classes + 1)\n",
        "summary(tunned_model, input_data=[batch])"
      ],
      "metadata": {
        "id": "XNO7oRTHZu6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка датасета для дообучения модели"
      ],
      "metadata": {
        "id": "PAvR1MgHrD9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "image_feeder = lambda: generate_objects_image(\n",
        "    objects=dataset,\n",
        "    size=(768, 1280, 3),\n",
        "    N=4,\n",
        "    transformation=transform\n",
        ")\n",
        "\n",
        "output_folder = './output'\n",
        "if os.path.isdir(output_folder):\n",
        "    shutil.rmtree(output_folder)\n",
        "os.mkdir(output_folder)\n",
        "\n",
        "N = 200\n",
        "part = 10\n",
        "learn_dataset = []\n",
        "for i in range(0, N):\n",
        "    image, boxes, image_labels = image_feeder()\n",
        "    image_file = f'img{i}.png'\n",
        "    image_path = f'{output_folder}/{image_file}'\n",
        "    image_data = Image.fromarray(image.astype(numpy.uint8))\n",
        "    image_data.save(image_path)\n",
        "    for k in range(0, len(boxes)):\n",
        "        learn_dataset.append({\n",
        "            'image': image_file,\n",
        "            'cx': boxes[k].cx/image.shape[1],\n",
        "            'cy': boxes[k].cy/image.shape[0],\n",
        "            'w': boxes[k].w/image.shape[1],\n",
        "            'h': boxes[k].h/image.shape[0],\n",
        "            'label': image_labels[k]\n",
        "        })\n",
        "    if i % part == 0:\n",
        "        print(f'{int(i/N*100)}% ({i}/{N}) were generated')\n",
        "pandas.DataFrame.from_records(learn_dataset).to_csv(\n",
        "    f'{output_folder}/labels.txt', index=False)\n",
        "shutil.make_archive('dataset', 'zip', root_dir=output_folder)\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "uyPPXnO-NQcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "см. https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6"
      ],
      "metadata": {
        "id": "GShz_txeqiHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output = []\n",
        "# def hook()\n",
        "# model.roi_heads.Conv2dNormActivation.ReLU.register_module_forward_hook(hook)"
      ],
      "metadata": {
        "id": "Cx8dqDPTnJz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import typing\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "LabeledImageSample = namedtuple('LabeledImageSample', ['image', 'labels'])\n",
        "xf = ['cx', 'w']\n",
        "yf = ['cy', 'h']\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, folder: str, device: torch.device,\n",
        "                 scale_bbox=False, num=None) -> None:\n",
        "        self.__folder = folder\n",
        "        self.__image = {}\n",
        "        self.__bbox = {}\n",
        "        self.__device = device\n",
        "        path = os.path.join(folder, 'labels.txt')\n",
        "        self.meta = pandas.read_csv(path, sep=',')\n",
        "        self.image_files = self.meta['image'].unique()\n",
        "        self.__num = num\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.__num is not None:\n",
        "            return self.__num\n",
        "\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, item: int) -> pandas.DataFrame:\n",
        "        file = self.image_files[item]\n",
        "        index = self.meta['image'] == file\n",
        "        if file not in self.__image:\n",
        "            self.__image[file] = torchvision.io.read_image(os.path.join(\n",
        "                self.__folder, file))\n",
        "            self.meta.loc[index, xf] = self.meta.loc[\n",
        "                index, xf]*self.__image[file].shape[2]\n",
        "            self.meta.loc[index, yf] = self.meta.loc[\n",
        "                index, yf]*self.__image[file].shape[1]\n",
        "            self.meta['p1x'] = self.meta['cx'] - self.meta['w']/2\n",
        "            self.meta['p2x'] = self.meta['cx'] + self.meta['w']/2\n",
        "            self.meta['p1y'] = self.meta['cy'] - self.meta['h']/2\n",
        "            self.meta['p2y'] = self.meta['cy'] + self.meta['h']/2\n",
        "\n",
        "        return LabeledImageSample(image=self.__image[file],\n",
        "                                  labels=self.meta.loc[index])"
      ],
      "metadata": {
        "id": "FHFMYg3rJzX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(folder=\"output\", device=device, num=None)"
      ],
      "metadata": {
        "id": "NWivBIMeU4_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = torchvision.transforms.functional.to_pil_image(img)\n",
        "        axs[0, i].imshow(numpy.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ],
      "metadata": {
        "id": "K1A-Mat_VXl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(data[33].image)"
      ],
      "metadata": {
        "id": "CtegX93kVh8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовка датасета для валидации обучения."
      ],
      "metadata": {
        "id": "wMdxI7rEuc3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_labels[0]['labels']['boxes'][0]['labels']['Object'][0]"
      ],
      "metadata": {
        "id": "J-C_6MNOvdd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "output_folder = 'validation'\n",
        "if os.path.exists(output_folder):\n",
        "    shutil.rmtree(output_folder)\n",
        "os.makedirs(output_folder)\n",
        "\n",
        "val_labels = []\n",
        "\n",
        "for item in validation_labels:\n",
        "    responce = requests.get(item['target'])\n",
        "    if not responce.status_code == 200:\n",
        "        raise Exception(\"Error, cant get image {}: {}\".format(\n",
        "            item['target'], responce.status_code))\n",
        "    fname = item['target'][(item['target'].rfind('/')+1):]\n",
        "    path = os.path.join(output_folder, fname)\n",
        "    with open(path, 'wb') as file:\n",
        "        file.write(responce.content)\n",
        "    for box in item['labels']['boxes']:\n",
        "        p1 = box['pt1']\n",
        "        p2 = box['pt2']\n",
        "        w = p2['x'] - p1['x']\n",
        "        h = p2['y'] - p1['y']\n",
        "        val_labels.append({\n",
        "            'image': fname,\n",
        "            'cx': p1['x'] + w/2,\n",
        "            'cy': p1['y'] + h/2,\n",
        "            'w': w,\n",
        "            'h': h,\n",
        "            'label': dataset.label2class[box['labels']['Object'][0]]\n",
        "        })\n",
        "pandas.DataFrame.from_records(val_labels).to_csv(\n",
        "    os.path.join(output_folder, 'labels.txt'))"
      ],
      "metadata": {
        "id": "3Inv1geMuhbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = Dataset(folder=\"validation\", device=device, num=None)"
      ],
      "metadata": {
        "id": "FGjGylT-4bJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(validation_data[0].image)"
      ],
      "metadata": {
        "id": "4lCJYZF64p3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дообучение модели Fastern RCNN для детекции новых классов"
      ],
      "metadata": {
        "id": "0PMmnoNZx1Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/johschmidt42/PyTorch-Object-Detection-Faster-RCNN-Tutorial"
      ],
      "metadata": {
        "id": "VBRrAxrJz-Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "losses = ['loss_classifier', 'loss_box_reg']\n",
        "\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, model, classes, device, preprocess, labels,\n",
        "                 scheduler=None, lr=0.1, modify=True) -> None:\n",
        "        self.__device = device\n",
        "        self.__orig_model = model\n",
        "        self.__tunned_model = copy.deepcopy(self.__orig_model)\n",
        "        if modify is True:\n",
        "            self.__modify(classes=classes)\n",
        "        self.__tunned_model.to(self.__device)\n",
        "        self.__preprocess = preprocess\n",
        "        self.__critery = torchvision.ops.complete_box_iou_loss\n",
        "        self.__optimizer = torch.optim.SGD(\n",
        "            self.__tunned_model.parameters(), lr=lr, momentum=0.9)\n",
        "        if scheduler is None:\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                self.__optimizer, step_size=3, gamma=0.5,verbose=True)\n",
        "        self.__scheduler = scheduler\n",
        "        self.__labels = labels\n",
        "\n",
        "    def __modify(self, classes: int, retrain_all: bool = False):\n",
        "        model = self.__tunned_model\n",
        "        if retrain_all is False:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        output = torchvision.models.detection.faster_rcnn\n",
        "        model.roi_heads.box_predictor = output.FastRCNNPredictor(\n",
        "            in_features, classes)\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.__tunned_model\n",
        "\n",
        "    def __prepare_inputs(self,\n",
        "                         item: LabeledImageSample):\n",
        "        inputs = item.image.float().to(self.__device)/255\n",
        "        labels = {\n",
        "            'boxes': torch.from_numpy(\n",
        "                item.labels[['p1x', 'p1y', 'p2x', 'p2y']].to_numpy()\n",
        "                ).to(self.__device),\n",
        "            'labels': torch.from_numpy(\n",
        "                item.labels['label'].to_numpy()).to(self.__device)\n",
        "        }\n",
        "        if len(inputs.shape) < 4:\n",
        "            inputs = inputs.reshape(-1, \n",
        "                                    inputs.shape[0],\n",
        "                                    inputs.shape[1],\n",
        "                                    inputs.shape[2])\n",
        "            labels = [labels]\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def train(self,\n",
        "              train_data: torch.utils.data.DataLoader,\n",
        "              epoch_count: int,\n",
        "              print_info: bool = True,\n",
        "              validation_data: torch.utils.data.DataLoader = None):\n",
        "        since = time.time()\n",
        "        self.__tunned_model.train()\n",
        "        loss_log = []\n",
        "        for i in range(epoch_count):\n",
        "            if print_info:\n",
        "                print(f\"Epoch {i+1} / {epoch_count}\")\n",
        "            result_loss = {}\n",
        "            validation_loss = {}\n",
        "            for key in losses:\n",
        "                result_loss[key] = 0\n",
        "                validation_loss[key] = 0\n",
        "\n",
        "            train_count = 0\n",
        "            for item in train_data:\n",
        "                train_count = train_count + 1\n",
        "                inputs, labels = self.__prepare_inputs(item)\n",
        "                self.__optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    loss = self.__tunned_model(inputs, targets=labels)\n",
        "                    for key in losses:\n",
        "                        result_loss[key] = result_loss[key] + loss[key].detach().cpu()\n",
        "                    loss_bw = loss['loss_classifier'] + loss['loss_box_reg']\n",
        "                    loss_bw.backward()\n",
        "                    self.__optimizer.step()\n",
        "\n",
        "            if validation_data is not None:\n",
        "                validation_count = 0\n",
        "                for item in validation_data:\n",
        "                    validation_count = validation_count + 1\n",
        "                    inputs, labels = self.__prepare_inputs(item)\n",
        "                    with torch.set_grad_enabled(False):\n",
        "                        loss = self.__tunned_model(inputs, targets=labels)\n",
        "                        for key in losses:\n",
        "                            validation_loss[key] = validation_loss[key] + loss[key].detach().cpu()\n",
        "\n",
        "            for key in losses:\n",
        "                result_loss[key] = result_loss[key]/train_count\n",
        "                print(f\"    [{i+1}/{epoch_count}][train][{key}] \" +\n",
        "                      f\"loss = {result_loss[key]}\")\n",
        "                if validation_data is not None:\n",
        "                    validation_loss[key] = validation_loss[key]/validation_count\n",
        "                    print(f\"    [{i+1}/{epoch_count}][validation][{key}] \" +\n",
        "                          f\"loss = {validation_loss[key]}\")\n",
        "            loss_log.append({'train': result_loss, 'validation': validation_loss})\n",
        "\n",
        "            self.__scheduler.step()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "        return loss_log\n",
        "                    \n",
        "            \n",
        "\n",
        "    def predict(self, inputs: torch.Tensor, measure_inference_time: bool = False):\n",
        "        if measure_inference_time:\n",
        "            since = time.time()\n",
        "        self.__tunned_model.eval()\n",
        "        result = self.__tunned_model(inputs)\n",
        "        labels = [self.__labels[i.detach().cpu().item()] for i in result[0]['labels']]\n",
        "\n",
        "        if measure_inference_time:\n",
        "            time_elapsed = time.time() - since\n",
        "            print(f'Inference has been completed in {time_elapsed // 60:.0f}m' +\n",
        "                  f' {time_elapsed % 60:.0f}s {int((time_elapsed - math.floor(time_elapsed))*1000)}ms')\n",
        "        return result, labels"
      ],
      "metadata": {
        "id": "kNxqv2pzyOqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.class2label\n",
        "new_model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5).to(device)\n",
        "tunned_model = Model(model=new_model, classes=3, device=device, preprocess=preprocess,\n",
        "                     labels=class_names, lr=0.025)"
      ],
      "metadata": {
        "id": "1zGoUyaxx2bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "WOE0CJzNowaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "hxu5hfZ8upUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_with_box_in_image(\n",
        "    image: numpy.ndarray,\n",
        "    labels:  typing.Union[typing.List[str], typing.Mapping[int, str]] = None,\n",
        "    meta: typing.Mapping[int, str] = None) -> None:\n",
        "    box = image\n",
        "\n",
        "    if labels is not None:\n",
        "        if meta is not None:\n",
        "            pred_labels = [meta[i] for i in labels[\"labels\"].tolist()]\n",
        "        else:\n",
        "            pred_labels = [str(i) for i in labels[\"labels\"]]\n",
        "        box = draw_bounding_boxes(box, boxes=torch.Tensor(labels[\"boxes\"]),\n",
        "                                  labels=pred_labels,\n",
        "                                  colors=\"red\",\n",
        "                                  width=4, font_size=30)\n",
        "\n",
        "    return box\n",
        "\n",
        "\n",
        "def draw_with_bbox(\n",
        "    image: numpy.ndarray,\n",
        "    labels:  typing.Union[typing.List[str], typing.Mapping[int, str]] = None,\n",
        "    meta: typing.Mapping[int, str] = None) -> None:\n",
        "\n",
        "    box = draw_with_box_in_image(image=image, labels=labels, meta=meta)\n",
        "\n",
        "    fig = plte.imshow(to_pil_image(box.detach()))\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def df2labels(df: pandas.DataFrame) -> typing.Mapping[str, typing.Union[\n",
        "    typing.List[numpy.array], typing.List[int]]]:\n",
        "    return {'boxes': df[['p1x', 'p1y', 'p2x', 'p2y']].to_numpy(),\n",
        "            'labels': df['label'].to_numpy()}\n"
      ],
      "metadata": {
        "id": "TIOtMnhivLPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0].image.shape"
      ],
      "metadata": {
        "id": "L_TJI-cZJUJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %debug\n",
        "draw_with_bbox(image=data[0].image, labels=df2labels(data[0].labels),\n",
        "               meta=class_names)"
      ],
      "metadata": {
        "id": "ojObSc5xuVR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %debug\n",
        "loss_log = tunned_model.train(\n",
        "    train_data=DataLoader(data,\n",
        "                          batch_size=None,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0),\n",
        "    epoch_count=15,\n",
        "    validation_data=validation_data\n",
        "    )"
      ],
      "metadata": {
        "id": "ytPPp0Q22Prx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learn_metrics(loss_log) -> None:\n",
        "    loss = pandas.DataFrame({\n",
        "        'train_box': [loss['train']['loss_box_reg'].item() for loss in loss_log],\n",
        "        'train_class': [loss['train']['loss_classifier'].item() for loss in loss_log],\n",
        "        'validation_box': [loss['validation']['loss_box_reg'].item() for loss in loss_log],\n",
        "        'validation_class': [loss['validation']['loss_classifier'].item() for loss in loss_log]\n",
        "        })\n",
        "    fig = make_subplots(\n",
        "                horizontal_spacing=0.1, vertical_spacing=0.1,\n",
        "                rows=2, cols=2, # shared_yaxes=True,\n",
        "                shared_xaxes=True,\n",
        "                subplot_titles=[\n",
        "                    \"Ошибка локализации (обучение)\",\n",
        "                    \"Ошибка классификации (обучение)\",\n",
        "                    \"Ошибка локализации (валидация)\",\n",
        "                    \"Ошибка классификации (валидация)\",\n",
        "                    ])\n",
        "    fig.add_trace(go.Scatter(x=loss.index, y=loss['train_box']), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(x=loss.index, y=loss['train_class']), row=1, col=2)\n",
        "    fig.add_trace(go.Scatter(x=loss.index, y=loss['validation_box']), row=2, col=1)\n",
        "    fig.add_trace(go.Scatter(x=loss.index, y=loss['validation_class']), row=2, col=2)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "v7tj0WopjyXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learn_metrics(loss_log)"
      ],
      "metadata": {
        "id": "2GTBNBnhQobi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(0, len(data)):\n",
        "#    pred = tunned_model.predict([(data[1].image.float()/255).to(device)])\n",
        "#    if len(pred[0][0]['boxes']) > 0:\n",
        "#        print(f\"image {i} has {len(pred[0][0]['boxes'])} boxes\")"
      ],
      "metadata": {
        "id": "YQCYEHgUohDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 2\n",
        "pred = tunned_model.predict([(data[i].image.float()/255).to(device)])\n",
        "draw_with_bbox(data[i].image, pred[0][0], meta=class_names)\n",
        "pred"
      ],
      "metadata": {
        "id": "zr8O9m4sMNzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tunned_model.get_model().roi_heads.score_thresh = 0.75\n",
        "first = list(dataset.basic_images.keys())[2]\n",
        "image = torch.Tensor(dataset.basic_images[first]).permute([2, 0, 1])/255\n",
        "pred = tunned_model.predict([image.to(device)])\n",
        "pred"
      ],
      "metadata": {
        "id": "f5sjFxljTZyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_with_bbox((image*255).to(torch.uint8), pred[0][0], meta=class_names)"
      ],
      "metadata": {
        "id": "2-lpD-oCQmAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "tunned_model.get_model().roi_heads.score_thresh = 0.75\n",
        "image = torch.Tensor(validation_data[i].image)/255 #.permute([2, 0, 1])/255\n",
        "pred = tunned_model.predict([image.to(device)])\n",
        "draw_with_bbox((image*255).to(torch.uint8), pred[0][0], meta=class_names)\n",
        "pred\n"
      ],
      "metadata": {
        "id": "VqghgS7cIf0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание видео файла с результатами распознавания"
      ],
      "metadata": {
        "id": "BE_ng2WFmY4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install video-cli"
      ],
      "metadata": {
        "id": "koebUeZObXp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O video-5.mp4 'https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/video_4/video_4.mp4'"
      ],
      "metadata": {
        "id": "J346Tv22mYS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "def process_video(input: str, model: typing.Callable[[torch.Tensor], None],\n",
        "                  output: str) -> None:\n",
        "    \"\"\" Function processes an input video file by a model and create an output \n",
        "        video file.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    step = int(length / 10)\n",
        "    ret, frame = cap.read()\n",
        "    h, w, _ = frame.shape\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"VP90\")\n",
        "    writer = cv2.VideoWriter(output, fourcc, fps, (w, h))\n",
        "\n",
        "    count = 1\n",
        "    while ret: \n",
        "        frame = model(frame)\n",
        "        writer.write(frame)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if count % step == 0:\n",
        "            print(f'{int(count/length*100)}% frames processed ({count}/{length})')\n",
        "                        \n",
        "        count += 1\n",
        "\n",
        "    writer.release()\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "buTgKoqdnJfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_box(image: numpy.ndarray,\n",
        "            device: torch.device,\n",
        "            model: typing.Callable[[typing.Any], typing.Tuple],\n",
        "            meta: typing.Mapping[int, str] = None) -> None:\n",
        "    imag = torch.Tensor(\n",
        "        numpy.transpose(image, axes=[2, 0, 1]).astype(float)).to(device)/255\n",
        "    pred = model.predict([imag])\n",
        "    return draw_with_box_in_image(image=(imag*255).byte(),\n",
        "                                  labels=pred[0][0],\n",
        "                                  meta=meta).cpu().numpy().transpose(1, 2, 0)"
      ],
      "metadata": {
        "id": "JpzQzP1WnAvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "model_operator = functools.partial(map_box, device=device,\n",
        "                                   model=tunned_model, meta=class_names)"
      ],
      "metadata": {
        "id": "f64CeVwvwI4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_video(input='video-5.mp4',\n",
        "              model=model_operator, output='video-5-out-1.webm')"
      ],
      "metadata": {
        "id": "sckoxmjFhknF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "# Video('video-5-out-1.webm', width=600, embed=True)"
      ],
      "metadata": {
        "id": "TOEyUjdujYhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Устранение артефактов в обучающей выборке\n",
        "\n",
        "1.   Новый пункт\n",
        "2.   Новый пункт\n",
        "\n"
      ],
      "metadata": {
        "id": "dmNouiXadeVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O object_masks_fixed_images.json 'https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/object_masks_fixed_images.json'"
      ],
      "metadata": {
        "id": "AIPi2Q31i1KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_labels = None\n",
        "with open('object_masks_fixed_images.json', 'r') as file:\n",
        "    fixed_labels = file.read()\n",
        "fixed_label_data = json.loads(fixed_labels)\n",
        "labeler = qsl.MediaLabeler(\n",
        "    items=fixed_label_data,\n",
        "    config={\n",
        "        \"regions\": [\n",
        "            {\"name\": \"Type\", \"multiple\": True, \"options\": [{\"name\": \"bottle\"},\n",
        "                                                           {\"name\": \"bag\"}]}\n",
        "        ]\n",
        "    })\n",
        "labeler.labels\n",
        "labeler.mode = \"dark\"\n",
        "labeler"
      ],
      "metadata": {
        "id": "57w8plEIjTkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_labels = [Struct(label) for label in fixed_label_data]\n",
        "fixed_dataset = MaskedObjectsDataset(metadata=fixed_labels)\n",
        "fixed_image_feeder = lambda: generate_objects_image(\n",
        "    objects=fixed_dataset,\n",
        "    size=(768, 1280, 3),\n",
        "    N=4,\n",
        "    transformation=transform\n",
        ")\n",
        "\n",
        "output_folder = './output_fixed'\n",
        "if os.path.isdir(output_folder):\n",
        "    shutil.rmtree(output_folder)\n",
        "os.mkdir(output_folder)\n",
        "\n",
        "N = 200\n",
        "part = 10\n",
        "learn_dataset = []\n",
        "for i in range(0, N):\n",
        "    image, boxes, image_labels = fixed_image_feeder()\n",
        "    image_file = f'img{i}.png'\n",
        "    image_path = f'{output_folder}/{image_file}'\n",
        "    image_data = Image.fromarray(image.astype(numpy.uint8))\n",
        "    image_data.save(image_path)\n",
        "    for k in range(0, len(boxes)):\n",
        "        learn_dataset.append({\n",
        "            'image': image_file,\n",
        "            'cx': boxes[k].cx/image.shape[1],\n",
        "            'cy': boxes[k].cy/image.shape[0],\n",
        "            'w': boxes[k].w/image.shape[1],\n",
        "            'h': boxes[k].h/image.shape[0],\n",
        "            'label': image_labels[k]\n",
        "        })\n",
        "    if i % part == 0:\n",
        "        print(f'{int(i/N*100)}% ({i}/{N}) were generated')\n",
        "pandas.DataFrame.from_records(learn_dataset).to_csv(\n",
        "    f'{output_folder}/labels.txt', index=False)\n",
        "shutil.make_archive('dataset_fixed', 'zip', root_dir=output_folder)\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "nol-j70fkJOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_marks_data = Dataset(folder=\"output_fixed\", device=device, num=None)"
      ],
      "metadata": {
        "id": "9a5HLSWNdkVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.class2label\n",
        "new_model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5).to(device)\n",
        "fixed_marks_tunned_model = Model(model=new_model, classes=3, device=device, preprocess=preprocess,\n",
        "                     labels=class_names, lr=0.025)\n",
        "# %debug\n",
        "corrected_loss_log = fixed_marks_tunned_model.train(\n",
        "    train_data=DataLoader(fixed_marks_data,\n",
        "                          batch_size=None,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0),\n",
        "    epoch_count=15,\n",
        "    validation_data=validation_data\n",
        "    )"
      ],
      "metadata": {
        "id": "8HDk8gAeinUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learn_metrics(corrected_loss_log)"
      ],
      "metadata": {
        "id": "rym5As2nQ6gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "fixed_model_operator = functools.partial(map_box, device=device,\n",
        "                                         model=fixed_marks_tunned_model, meta=class_names)\n",
        "process_video(input='video-5.mp4',\n",
        "              model=fixed_model_operator, output='video-5-out-marks-fixed.webm')"
      ],
      "metadata": {
        "id": "6kNtv73IWqD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение на изображениях со специфичными фонами"
      ],
      "metadata": {
        "id": "HRvnVYlzQ6uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile images-background.csv\n",
        "target\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/background/image_001.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/background/image_005.jpg\n",
        "https://github.com/ant-nik/neural_network_course/raw/main/practice_2_data/background/image_011.jpg"
      ],
      "metadata": {
        "id": "QhmM1cbqQ6CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_validation_file = pandas.read_csv('images-background.csv')\n",
        "image_validation_file"
      ],
      "metadata": {
        "id": "e7C50gjeWyzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_background(backgrounds: typing.Sequence[numpy.ndarray],\n",
        "                        size: typing.Tuple[int, int, int]):\n",
        "    transform = aug.Compose([\n",
        "        aug.CropAndPad(percent=(1.0, 1.0), keep_size=False,\n",
        "                   pad_mode=cv2.BORDER_REFLECT),\n",
        "        # aug.LongestMaxSize(max_size=300),\n",
        "        aug.Rotate(p=1.0, border_mode=cv2.BORDER_REFLECT,\n",
        "                value=(0, 0, 0), mask_value=(0, 0, 0)),\n",
        "        #aug.HorizontalFlip(p=0.5),\n",
        "        #aug.VerticalFlip(p=0.5),\n",
        "        aug.RandomScale(scale_limit=0.5, always_apply=True),\n",
        "        aug.RandomCrop(width=size[0], height=size[1], p=1.0),\n",
        "        aug.MotionBlur(),\n",
        "        aug.RGBShift(always_apply=True,\n",
        "                    r_shift_limit=20, g_shift_limit=20, b_shift_limit=20),\n",
        "        aug.HueSaturationValue(always_apply=True, p=1.0),\n",
        "        aug.RandomBrightnessContrast(p=0.2),\n",
        "        aug.GaussNoise(always_apply=True)\n",
        "    ])"
      ],
      "metadata": {
        "id": "IVwr7FOkW9hy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}