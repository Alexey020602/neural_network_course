{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGDpNlq7b3Rf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "\n",
        "import typing\n",
        "from typing import Callable\n",
        "from functools import partial\n",
        "\n",
        "import numpy\n",
        "import pandas\n",
        "from pandas.core.arrays import boolean\n",
        "import matplotlib.pyplot as plt\n",
        "from requests import get\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import ToPILImage, Compose, Resize, CenterCrop\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "strip_chars = ' \\t'\n",
        "tmp_file_name = 'tmp_file_name_for_image_download'\n",
        "\n",
        "to_image = ToPILImage()\n",
        "\n",
        "\n",
        "def classify(dataset: Dataset,\n",
        "                preprocess: typing.Callable[[Tensor],Tensor],\n",
        "                num_per_row: int, single_size: float,\n",
        "                labels: typing.List[str],\n",
        "                model_labels: typing.List[str] = None,\n",
        "                model: typing.Callable[[Tensor], Tensor] = None,\n",
        "                debug: typing.Any = False,\n",
        "                num_of_classes: int = 1,\n",
        "                vspace: float = 0.3\n",
        "             ) -> None:\n",
        "    num = len(dataset)\n",
        "    fig, axs = plt.subplots(math.ceil(num/num_per_row), num_per_row, figsize=(\n",
        "        single_size*num_per_row, (single_size + vspace)*(math.ceil(num/num_per_row))),\n",
        "        sharex=True, sharey=True)\n",
        "    for i in range(0, len(dataset)):\n",
        "        try:\n",
        "            image, label = dataset[i]\n",
        "            pred = None\n",
        "\n",
        "            if model is not None and model_labels is not None:\n",
        "                start_time = time.perf_counter_ns()\n",
        "                score = model(image.unsqueeze(0)).detach().squeeze(0).softmax(0)\n",
        "                pred_index = numpy.flip(score.detach().cpu().argsort().numpy())[0]\n",
        "                end_time = time.perf_counter_ns()\n",
        "                pred = f'Detector: {(end_time - start_time) / 1_000_000:.0f}ms\\n{model_labels[pred_index]}[{score[pred_index].item()*100:.0f}%]'\n",
        "\n",
        "            loc_fig = axs[i//num_per_row, i % num_per_row]\n",
        "            loc_fig.imshow(to_image(preprocess(image)))\n",
        "            title =f'\\nActual: {labels[label]}[{label}]\\n{pred}'\n",
        "            loc_fig.title.set_text(title)\n",
        "        except Exception as ex:\n",
        "            if debug:\n",
        "                raise ex\n",
        "            print(f'Image {i} is failed to load: {str(ex)}')\n",
        "\n",
        "    fig.subplots_adjust(wspace=0.3)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def denormalize(dataset: Dataset, trans: typing.Any) -> Callable[[Tensor], Tensor]:\n",
        "    image, label = dataset[0]\n",
        "    std = torch.as_tensor(trans.std, dtype=image.dtype, device=image.device).view(-1, 1, 1)\n",
        "    mean = torch.as_tensor(trans.mean, dtype=image.dtype, device=image.device).view(-1, 1, 1)\n",
        "    return lambda img: img*std + mean\n",
        "\n",
        "\n",
        "class UrlDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file: str, to_device, transform = None) -> None:\n",
        "        self.file = file\n",
        "        self.transform = transform\n",
        "        self.dataset = pandas.read_csv(file, sep=';')\n",
        "        self.classes = self.dataset['label'].unique()\n",
        "        self.classes.sort()\n",
        "        self.class_to_index = {self.classes[i] : i for i in range(len(self.classes))}\n",
        "        self.device = to_device\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> typing.Tuple[torch.Tensor, str]:\n",
        "        url = self.dataset.iloc[index]['url'].strip(strip_chars)\n",
        "        with open(tmp_file_name, 'wb') as file:\n",
        "            file.write(get(url).content)\n",
        "        image = read_image(tmp_file_name).to(self.device)\n",
        "        label = self.class_to_index[self.dataset.iloc[index]['label']]\n",
        "        if self.transform:\n",
        "            return self.transform(image), label\n",
        "        return image, label\n",
        "\n",
        "\n",
        "def train_model(model, dataloader: DataLoader, device: torch.device,\n",
        "                critery, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 0.0\n",
        "    best_epoch = -1\n",
        "\n",
        "    process = {'train': {'loss': [], 'accuracy': []}, 'validate': {'loss': [], 'accuracy': []}}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc = 0.0\n",
        "        for item in dataloader:\n",
        "            if item['train'] == True:\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            dataset_size = 0\n",
        "            for inputs, labels in item['loader']:\n",
        "                dataset_size = dataset_size + 1\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(item['train'] == True):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = critery(outputs, labels)\n",
        "                    if item['train'] == True:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if item['train'] == True:\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.detach().cpu().double() / dataset_size\n",
        "\n",
        "            if item['train'] == True:\n",
        "                ptype = 'train'\n",
        "            else:\n",
        "                ptype = 'validate'\n",
        "\n",
        "            process[ptype]['loss'].append(epoch_loss)\n",
        "            process[ptype]['accuracy'].append(epoch_acc)\n",
        "\n",
        "            print(f'[{epoch}][train={item[\"train\"]}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if item['train'] == True and 1/epoch_loss > best_loss:\n",
        "                best_loss = 1/epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                best_epoch = epoch\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val loss: {1/best_loss:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "15ytPBPscrrM"
      }
    }
  ]
}