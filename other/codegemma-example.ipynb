{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwe/HfSGPRsiErRej4FhIs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code generation with LLM examples\n",
        "\n",
        "This tutorial is based on [Ollama](https://github.com/ollama/ollama) project to easily pull/run/learn LLM models.\n",
        "\n",
        "Let's download Ollama portable binary first."
      ],
      "metadata": {
        "id": "Idn5VMHUfN1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FAWp4E2LL_Y"
      },
      "outputs": [],
      "source": [
        "!curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "!chmod +x ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collab and other Jupyter-environment is not intended to run a background services, so it is required to use a Python code that runs Ollama."
      ],
      "metadata": {
        "id": "xzlvrvuatCik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"./ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "9z1Xr7gVLStp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start generating code it is required to pull appropriate LLM model. One can use [Codegemma](https://ollama.com/library/codegemma) from Ollama. It requires ~10Gb RAM to make a generation and takes up to 8 mintes in Collab without GPU. With a GPU installed a prediction will be faster."
      ],
      "metadata": {
        "id": "_uA1DkLJtKMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./ollama pull codegemma"
      ],
      "metadata": {
        "id": "XcDUfvbJLcQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a task for our LLM and save it in a file."
      ],
      "metadata": {
        "id": "R1wnR1Zruz1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile codegemma.txt\n",
        "Generate a Python code to plot a count of objects in the LIST that have values above VALUE.\n",
        "Input has following Python structure: [{\\\"score\\\": LIST}].\n",
        "Use plotly as plot library."
      ],
      "metadata": {
        "id": "f9ciH-hkLgFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ollama and its LLMs expects a prompt in JSON format, so it is required to prepare a request that is based on a text prompt above."
      ],
      "metadata": {
        "id": "OMwxDqTFu_xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo '{ \"model\": \"codegemma\", \"prompt\": \"'`cat codegemma.txt`'\", \"stream\": false}' > codegemma_request_body.json\n",
        "!cat codegemma.txt"
      ],
      "metadata": {
        "id": "J-BupO9pLmpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a code generation!"
      ],
      "metadata": {
        "id": "ggEQDzQMvUDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --data-binary \"@codegemma_request_body.json\" -o codegemma_reply.json http://localhost:11434/api/generate\n",
        "!cat codegemma_reply.json"
      ],
      "metadata": {
        "id": "Ntoejr64LVpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A responce from a model is also a JSON, so it is required to prepare some function to print only a generated answer."
      ],
      "metadata": {
        "id": "bMETyMIjvfmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def json_to_dict(filename: str) -> dict[str, any]:\n",
        "    with open(filename, \"r\") as file:\n",
        "        step2_response = json.loads(file.read())\n",
        "    return step2_response[\"response\"] if \"responce\" in step2_response else step2_response"
      ],
      "metadata": {
        "id": "2n4I2zZsNZ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print a generated answer."
      ],
      "metadata": {
        "id": "6jlQrQDfvqUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(json_to_dict(\"codegemma_reply.json\")[\"response\"])"
      ],
      "metadata": {
        "id": "jQjBCkQrLZra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}